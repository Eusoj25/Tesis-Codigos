wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
wandb: Adding directory to artifact (C:\Users\PAVILION 15-DW001LA\Documents\Tesis\CÃ³digos\RNAs\Red_Densa\wandb\run-20230404_112919-w5aclw8j\files\model-best)... Done. 0.0s
Epoch 1/30
33/40 [=======================>......] - ETA: 0s - loss: 0.0397 - mse: 0.0397
40/40 [==============================] - 1s 25ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0032 - val_mse: 0.0032
Epoch 2/30
35/40 [=========================>....] - ETA: 0s - loss: 0.0021 - mse: 0.0021
40/40 [==============================] - 1s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0012 - val_mse: 0.0012
Epoch 3/30
37/40 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010
40/40 [==============================] - 1s 23ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 8.2449e-04 - val_mse: 8.2449e-04
Epoch 4/30
37/40 [==========================>...] - ETA: 0s - loss: 7.3793e-04 - mse: 7.3793e-04
40/40 [==============================] - 1s 20ms/step - loss: 7.3811e-04 - mse: 7.3811e-04 - val_loss: 6.4846e-04 - val_mse: 6.4846e-04
Epoch 5/30
37/40 [==========================>...] - ETA: 0s - loss: 5.9676e-04 - mse: 5.9676e-04
40/40 [==============================] - 1s 20ms/step - loss: 5.9727e-04 - mse: 5.9727e-04 - val_loss: 5.3900e-04 - val_mse: 5.3900e-04
Epoch 6/30
37/40 [==========================>...] - ETA: 0s - loss: 5.1081e-04 - mse: 5.1081e-04
40/40 [==============================] - 2s 41ms/step - loss: 5.0429e-04 - mse: 5.0429e-04 - val_loss: 4.6355e-04 - val_mse: 4.6355e-04
Epoch 7/30
38/40 [===========================>..] - ETA: 0s - loss: 4.4204e-04 - mse: 4.4204e-04
40/40 [==============================] - 1s 19ms/step - loss: 4.3872e-04 - mse: 4.3872e-04 - val_loss: 4.0723e-04 - val_mse: 4.0723e-04
Epoch 8/30
37/40 [==========================>...] - ETA: 0s - loss: 3.9705e-04 - mse: 3.9705e-04
40/40 [==============================] - 1s 20ms/step - loss: 3.8918e-04 - mse: 3.8918e-04 - val_loss: 3.6202e-04 - val_mse: 3.6202e-04
Epoch 9/30
35/40 [=========================>....] - ETA: 0s - loss: 3.5648e-04 - mse: 3.5648e-04
40/40 [==============================] - 1s 20ms/step - loss: 3.4899e-04 - mse: 3.4899e-04 - val_loss: 3.2566e-04 - val_mse: 3.2566e-04
Epoch 10/30
38/40 [===========================>..] - ETA: 0s - loss: 3.1818e-04 - mse: 3.1818e-04
40/40 [==============================] - 1s 19ms/step - loss: 3.1684e-04 - mse: 3.1684e-04 - val_loss: 2.9756e-04 - val_mse: 2.9756e-04
Epoch 11/30
39/40 [============================>.] - ETA: 0s - loss: 2.9085e-04 - mse: 2.9085e-04
40/40 [==============================] - 1s 19ms/step - loss: 2.9017e-04 - mse: 2.9017e-04 - val_loss: 2.7269e-04 - val_mse: 2.7269e-04
Epoch 12/30
37/40 [==========================>...] - ETA: 0s - loss: 2.6605e-04 - mse: 2.6605e-04
40/40 [==============================] - 1s 22ms/step - loss: 2.6734e-04 - mse: 2.6734e-04 - val_loss: 2.5096e-04 - val_mse: 2.5096e-04
Epoch 13/30
37/40 [==========================>...] - ETA: 0s - loss: 2.4931e-04 - mse: 2.4931e-04
40/40 [==============================] - 1s 20ms/step - loss: 2.4766e-04 - mse: 2.4766e-04 - val_loss: 2.3334e-04 - val_mse: 2.3334e-04
Epoch 14/30
40/40 [==============================] - ETA: 0s - loss: 2.3090e-04 - mse: 2.3090e-04
40/40 [==============================] - 1s 24ms/step - loss: 2.3090e-04 - mse: 2.3090e-04 - val_loss: 2.1738e-04 - val_mse: 2.1738e-04
Epoch 15/30
36/40 [==========================>...] - ETA: 0s - loss: 2.2011e-04 - mse: 2.2011e-04
40/40 [==============================] - 1s 20ms/step - loss: 2.1584e-04 - mse: 2.1584e-04 - val_loss: 2.0412e-04 - val_mse: 2.0412e-04
Epoch 16/30
37/40 [==========================>...] - ETA: 0s - loss: 2.0451e-04 - mse: 2.0451e-04
40/40 [==============================] - 1s 19ms/step - loss: 2.0227e-04 - mse: 2.0227e-04 - val_loss: 1.9125e-04 - val_mse: 1.9125e-04
Epoch 17/30
40/40 [==============================] - 1s 19ms/step - loss: 1.9048e-04 - mse: 1.9048e-04 - val_loss: 1.7883e-04 - val_mse: 1.7883e-04
Epoch 18/30
17/40 [===========>..................] - ETA: 0s - loss: 1.8223e-04 - mse: 1.8223e-04

35/40 [=========================>....] - ETA: 0s - loss: 1.8063e-04 - mse: 1.8063e-04
40/40 [==============================] - 1s 21ms/step - loss: 1.7954e-04 - mse: 1.7954e-04 - val_loss: 1.6834e-04 - val_mse: 1.6834e-04
Epoch 19/30
39/40 [============================>.] - ETA: 0s - loss: 1.7011e-04 - mse: 1.7011e-04
40/40 [==============================] - 1s 18ms/step - loss: 1.6969e-04 - mse: 1.6969e-04 - val_loss: 1.5951e-04 - val_mse: 1.5951e-04
Epoch 20/30
36/40 [==========================>...] - ETA: 0s - loss: 1.6326e-04 - mse: 1.6326e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.6071e-04 - mse: 1.6071e-04 - val_loss: 1.5088e-04 - val_mse: 1.5088e-04
Epoch 21/30
38/40 [===========================>..] - ETA: 0s - loss: 1.5130e-04 - mse: 1.5130e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.5218e-04 - mse: 1.5218e-04 - val_loss: 1.4294e-04 - val_mse: 1.4294e-04
Epoch 22/30
39/40 [============================>.] - ETA: 0s - loss: 1.4499e-04 - mse: 1.4499e-04
40/40 [==============================] - 1s 22ms/step - loss: 1.4471e-04 - mse: 1.4471e-04 - val_loss: 1.3597e-04 - val_mse: 1.3597e-04
Epoch 23/30
36/40 [==========================>...] - ETA: 0s - loss: 1.3683e-04 - mse: 1.3683e-04
40/40 [==============================] - 1s 19ms/step - loss: 1.3788e-04 - mse: 1.3788e-04 - val_loss: 1.2929e-04 - val_mse: 1.2929e-04
Epoch 24/30
35/40 [=========================>....] - ETA: 0s - loss: 1.3131e-04 - mse: 1.3131e-04
40/40 [==============================] - 1s 24ms/step - loss: 1.3147e-04 - mse: 1.3147e-04 - val_loss: 1.2369e-04 - val_mse: 1.2369e-04
Epoch 25/30
35/40 [=========================>....] - ETA: 0s - loss: 1.2419e-04 - mse: 1.2419e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.2573e-04 - mse: 1.2573e-04 - val_loss: 1.1854e-04 - val_mse: 1.1854e-04
Epoch 26/30
32/40 [=======================>......] - ETA: 0s - loss: 1.2186e-04 - mse: 1.2186e-04
40/40 [==============================] - 1s 19ms/step - loss: 1.2034e-04 - mse: 1.2034e-04 - val_loss: 1.1324e-04 - val_mse: 1.1324e-04
Epoch 27/30
33/40 [=======================>......] - ETA: 0s - loss: 1.1441e-04 - mse: 1.1441e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.1537e-04 - mse: 1.1537e-04 - val_loss: 1.0843e-04 - val_mse: 1.0843e-04
Epoch 28/30
35/40 [=========================>....] - ETA: 0s - loss: 1.1171e-04 - mse: 1.1171e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.1072e-04 - mse: 1.1072e-04 - val_loss: 1.0400e-04 - val_mse: 1.0400e-04
Epoch 29/30
34/40 [========================>.....] - ETA: 0s - loss: 1.0452e-04 - mse: 1.0452e-04
40/40 [==============================] - 1s 20ms/step - loss: 1.0649e-04 - mse: 1.0649e-04 - val_loss: 1.0032e-04 - val_mse: 1.0032e-04
Epoch 30/30
40/40 [==============================] - 1s 21ms/step - loss: 1.0244e-04 - mse: 1.0244e-04 - val_loss: 9.6658e-05 - val_mse: 9.6658e-05
7/7 [==============================] - 0s 2ms/step - loss: 9.6658e-05 - mse: 9.6658e-05
[9.665768448030576e-05, 9.665768448030576e-05]
Lo que obtuvo la red fue:
[0.07006863 0.03243467 0.03438163 0.07117239]
El resultado correcto:
[0.0689  0.03495 0.03595 0.0699 ]